{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angloher/projects/Predicting_Winning_Teams/PredictEnv/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#data preprocessing\n",
    "import pandas as pd\n",
    "#produces a prediction model in the form of an ensemble of weak prediction models, typically decision tree\n",
    "import xgboost as xgb\n",
    "#the outcome (dependent variable) has only a limited number of possible values. \n",
    "#Logistic Regression is used when response variable is categorical in nature.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#A random forest is a meta estimator that fits a number of decision tree classifiers \n",
    "#on various sub-samples of the dataset and use averaging to improve the predictive \n",
    "#accuracy and control over-fitting.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#a discriminative classifier formally defined by a separating hyperplane.\n",
    "from sklearn.svm import SVC\n",
    "#displayd data\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161940 161940\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AC</th>\n",
       "      <th>ACC</th>\n",
       "      <th>ACS</th>\n",
       "      <th>AM10Conceded</th>\n",
       "      <th>AM10OppConceded</th>\n",
       "      <th>AM10OppPoints</th>\n",
       "      <th>AM10OppScored</th>\n",
       "      <th>AM10Scored</th>\n",
       "      <th>AM11Conceded</th>\n",
       "      <th>...</th>\n",
       "      <th>LY4HSS</th>\n",
       "      <th>LY4HSTC</th>\n",
       "      <th>LY4HSTS</th>\n",
       "      <th>LY4HTGC</th>\n",
       "      <th>LY4HTGS</th>\n",
       "      <th>LY4HTP</th>\n",
       "      <th>MW</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 358 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  AC  ACC  ACS  AM10Conceded  AM10OppConceded  AM10OppPoints  \\\n",
       "0           0 NaN  NaN  NaN           NaN              NaN            NaN   \n",
       "1           1 NaN  NaN  NaN           NaN              NaN            NaN   \n",
       "2           2 NaN  NaN  NaN           NaN              NaN            NaN   \n",
       "3           3 NaN  NaN  NaN           NaN              NaN            NaN   \n",
       "4           4 NaN  NaN  NaN           NaN              NaN            NaN   \n",
       "\n",
       "   AM10OppScored  AM10Scored  AM11Conceded  ...    LY4HSS  LY4HSTC  LY4HSTS  \\\n",
       "0            NaN         NaN           NaN  ...       NaN      NaN      NaN   \n",
       "1            NaN         NaN           NaN  ...       NaN      NaN      NaN   \n",
       "2            NaN         NaN           NaN  ...       NaN      NaN      NaN   \n",
       "3            NaN         NaN           NaN  ...       NaN      NaN      NaN   \n",
       "4            NaN         NaN           NaN  ...       NaN      NaN      NaN   \n",
       "\n",
       "   LY4HTGC  LY4HTGS  LY4HTP  MW  B365H  B365D  B365A  \n",
       "0      NaN      NaN     NaN   1    NaN    NaN    NaN  \n",
       "1      NaN      NaN     NaN   1    NaN    NaN    NaN  \n",
       "2      NaN      NaN     NaN   1    NaN    NaN    NaN  \n",
       "3      NaN      NaN     NaN   1    NaN    NaN    NaN  \n",
       "4      NaN      NaN     NaN   1    NaN    NaN    NaN  \n",
       "\n",
       "[5 rows x 358 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "# Read data and drop redundant column.\n",
    "loc = \"/home/angloher/projects/Predicting_Winning_Teams/Datasets\" #('Scotish_league',4)\n",
    "leagues=[('Premier_league',5),\n",
    "        ('Bundesliga',2),('French_league',2),('Portoguese_league',1),('Turkish_league',1),\n",
    "         ('Greek_league',1),('Belgian_league',1),\n",
    "         ('La_Liga',2),('Eredivisie',1),('SerieA',2)]\n",
    "         #('Swiss_league',1),('Finish_league',1),('Austrian_league',1),\n",
    "         #('Scotish_league',4),('Chinese_league',1),('Norwegian_league',1),\n",
    "         #('Russian_league',1),('Swedish_league',1),('Brazilian_league',1),\n",
    "         #('Polish_league',1),('Romanian_league',1),('Mexican_league',1),\n",
    "         #('Japanese_league',1),('Danish_league',1)]#]\n",
    "train_val_split_date=['07','13','2018']#month/day/year\n",
    "#train_val_stop_date=['02','20','2018']\n",
    "files=[]\n",
    "betting_files=[]\n",
    "num_last_games=20\n",
    "num_last_seasons=6\n",
    "\n",
    "for league,divisions in leagues:\n",
    "    for i in range(1,divisions+1):\n",
    "        #print len(pd.read_csv(os.path.join(loc,league,str(i),'Results','final_dataset.csv')))\n",
    "        #print len(pd.read_csv(os.path.join(loc,league,str(i),'Results','betting_odds.csv')))\n",
    "        files.append(pd.read_csv(os.path.join(loc,league,str(i),'Results','final_dataset.csv')))\n",
    "        betting_files.append(pd.read_csv(os.path.join(loc,league,str(i),'Results','betting_odds.csv'))[['B365H','B365D','B365A']])\n",
    "data=pd.concat(files)\n",
    "betting_data=pd.concat(betting_files)\n",
    "\n",
    "print len(data),len(betting_data)\n",
    "data=pd.concat([data,betting_data],axis=1)\n",
    "\n",
    "display(data.head())\n",
    "#Full Time Result (H=Home Win, D=Draw, A=Away Win)\n",
    "#HTGD - Home team goal difference\n",
    "#ATGD - away team goal difference\n",
    "#HTP - Home team points\n",
    "#ATP - Away team points\n",
    "#DiffFormPts Diff in points\n",
    "#DiffLP - Differnece in last years prediction\n",
    "\n",
    "#Input - 12 other features (fouls, shots, goals, misses,corners, red card, yellow cards)\n",
    "#Output - Full Time Result (H=Home Win, D=Draw, A=Away Win) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of matches: 161940\n",
      "Number of features: 357\n",
      "Number of matches won by home team: 74593\n",
      "Win rate of home team: 46.06%\n"
     ]
    }
   ],
   "source": [
    "#what is the win rate for the home team?\n",
    "\n",
    "# Total number of matches.\n",
    "n_matches = data.shape[0]\n",
    "\n",
    "# Calculate number of features. -1 because we are saving one as the target variable (win/lose/draw)\n",
    "n_features = data.shape[1] - 1\n",
    "\n",
    "# Calculate matches won by home team.\n",
    "n_homewins = len(data[data.FTR == 'H'])\n",
    "\n",
    "# Calculate win rate for home team.\n",
    "win_rate = (float(n_homewins) / (n_matches)) * 100\n",
    "\n",
    "# Print the results\n",
    "print \"Total number of matches: {}\".format(n_matches)\n",
    "print \"Number of features: {}\".format(n_features)\n",
    "print \"Number of matches won by home team: {}\".format(n_homewins)\n",
    "print \"Win rate of home team: {:.2f}%\".format(win_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angloher/projects/Predicting_Winning_Teams/PredictEnv/local/lib/python2.7/site-packages/pandas/core/generic.py:3660: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/angloher/projects/Predicting_Winning_Teams/PredictEnv/lib/python2.7/site-packages/ipykernel_launcher.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/angloher/projects/Predicting_Winning_Teams/PredictEnv/lib/python2.7/site-packages/ipykernel_launcher.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/angloher/projects/Predicting_Winning_Teams/PredictEnv/lib/python2.7/site-packages/ipykernel_launcher.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160610 160610\n",
      "['Unnamed: 0', 'AC', 'ACC', 'ACS', 'AM10Conceded', 'AM10OppConceded', 'AM10OppPoints', 'AM10OppScored', 'AM10Scored', 'AM11Conceded', 'AM11OppConceded', 'AM11OppPoints', 'AM11OppScored', 'AM11Scored', 'AM12Conceded', 'AM12OppConceded', 'AM12OppPoints', 'AM12OppScored', 'AM12Scored', 'AM13Conceded', 'AM13OppConceded', 'AM13OppPoints', 'AM13OppScored', 'AM13Scored', 'AM14Conceded', 'AM14OppConceded', 'AM14OppPoints', 'AM14OppScored', 'AM14Scored', 'AM15Conceded', 'AM15OppConceded', 'AM15OppPoints', 'AM15OppScored', 'AM15Scored', 'AM16Conceded', 'AM16OppConceded', 'AM16OppPoints', 'AM16OppScored', 'AM16Scored', 'AM17Conceded', 'AM17OppConceded', 'AM17OppPoints', 'AM17OppScored', 'AM17Scored', 'AM18Conceded', 'AM18OppConceded', 'AM18OppPoints', 'AM18OppScored', 'AM18Scored', 'AM19Conceded', 'AM19OppConceded', 'AM19OppPoints', 'AM19OppScored', 'AM19Scored', 'AM1Conceded', 'AM1OppConceded', 'AM1OppPoints', 'AM1OppScored', 'AM1Scored', 'AM20Conceded', 'AM20OppConceded', 'AM20OppPoints', 'AM20OppScored', 'AM20Scored', 'AM2Conceded', 'AM2OppConceded', 'AM2OppPoints', 'AM2OppScored', 'AM2Scored', 'AM3Conceded', 'AM3OppConceded', 'AM3OppPoints', 'AM3OppScored', 'AM3Scored', 'AM4Conceded', 'AM4OppConceded', 'AM4OppPoints', 'AM4OppScored', 'AM4Scored', 'AM5Conceded', 'AM5OppConceded', 'AM5OppPoints', 'AM5OppScored', 'AM5Scored', 'AM6Conceded', 'AM6OppConceded', 'AM6OppPoints', 'AM6OppScored', 'AM6Scored', 'AM7Conceded', 'AM7OppConceded', 'AM7OppPoints', 'AM7OppScored', 'AM7Scored', 'AM8Conceded', 'AM8OppConceded', 'AM8OppPoints', 'AM8OppScored', 'AM8Scored', 'AM9Conceded', 'AM9OppConceded', 'AM9OppPoints', 'AM9OppScored', 'AM9Scored', 'AS', 'ASC', 'ASS', 'AST', 'ASTC', 'ASTS', 'ATGC', 'ATGS', 'ATP', 'AwayTeam', 'Date', 'Div', 'FTAG', 'FTHG', 'HC', 'HCC', 'HCS', 'HM10Conceded', 'HM10OppConceded', 'HM10OppPoints', 'HM10OppScored', 'HM10Scored', 'HM11Conceded', 'HM11OppConceded', 'HM11OppPoints', 'HM11OppScored', 'HM11Scored', 'HM12Conceded', 'HM12OppConceded', 'HM12OppPoints', 'HM12OppScored', 'HM12Scored', 'HM13Conceded', 'HM13OppConceded', 'HM13OppPoints', 'HM13OppScored', 'HM13Scored', 'HM14Conceded', 'HM14OppConceded', 'HM14OppPoints', 'HM14OppScored', 'HM14Scored', 'HM15Conceded', 'HM15OppConceded', 'HM15OppPoints', 'HM15OppScored', 'HM15Scored', 'HM16Conceded', 'HM16OppConceded', 'HM16OppPoints', 'HM16OppScored', 'HM16Scored', 'HM17Conceded', 'HM17OppConceded', 'HM17OppPoints', 'HM17OppScored', 'HM17Scored', 'HM18Conceded', 'HM18OppConceded', 'HM18OppPoints', 'HM18OppScored', 'HM18Scored', 'HM19Conceded', 'HM19OppConceded', 'HM19OppPoints', 'HM19OppScored', 'HM19Scored', 'HM1Conceded', 'HM1OppConceded', 'HM1OppPoints', 'HM1OppScored', 'HM1Scored', 'HM20Conceded', 'HM20OppConceded', 'HM20OppPoints', 'HM20OppScored', 'HM20Scored', 'HM2Conceded', 'HM2OppConceded', 'HM2OppPoints', 'HM2OppScored', 'HM2Scored', 'HM3Conceded', 'HM3OppConceded', 'HM3OppPoints', 'HM3OppScored', 'HM3Scored', 'HM4Conceded', 'HM4OppConceded', 'HM4OppPoints', 'HM4OppScored', 'HM4Scored', 'HM5Conceded', 'HM5OppConceded', 'HM5OppPoints', 'HM5OppScored', 'HM5Scored', 'HM6Conceded', 'HM6OppConceded', 'HM6OppPoints', 'HM6OppScored', 'HM6Scored', 'HM7Conceded', 'HM7OppConceded', 'HM7OppPoints', 'HM7OppScored', 'HM7Scored', 'HM8Conceded', 'HM8OppConceded', 'HM8OppPoints', 'HM8OppScored', 'HM8Scored', 'HM9Conceded', 'HM9OppConceded', 'HM9OppPoints', 'HM9OppScored', 'HM9Scored', 'HS', 'HSC', 'HSS', 'HST', 'HSTC', 'HSTS', 'HTGC', 'HTGS', 'HTP', 'HomeTeam', 'LMUP0AT', 'LMUP0HT', 'LMUP10AT', 'LMUP10HT', 'LMUP1AT', 'LMUP1HT', 'LMUP2AT', 'LMUP2HT', 'LMUP3AT', 'LMUP3HT', 'LMUP4AT', 'LMUP4HT', 'LMUP5AT', 'LMUP5HT', 'LMUP6AT', 'LMUP6HT', 'LMUP7AT', 'LMUP7HT', 'LMUP8AT', 'LMUP8HT', 'LMUP9AT', 'LMUP9HT', 'LY0ACC', 'LY0ACS', 'LY0ASC', 'LY0ASS', 'LY0ASTC', 'LY0ASTS', 'LY0ATGC', 'LY0ATGS', 'LY0ATP', 'LY0DivAT', 'LY0DivHT', 'LY0HCC', 'LY0HCS', 'LY0HSC', 'LY0HSS', 'LY0HSTC', 'LY0HSTS', 'LY0HTGC', 'LY0HTGS', 'LY0HTP', 'LY1ACC', 'LY1ACS', 'LY1ASC', 'LY1ASS', 'LY1ASTC', 'LY1ASTS', 'LY1ATGC', 'LY1ATGS', 'LY1ATP', 'LY1DivAT', 'LY1DivHT', 'LY1HCC', 'LY1HCS', 'LY1HSC', 'LY1HSS', 'LY1HSTC', 'LY1HSTS', 'LY1HTGC', 'LY1HTGS', 'LY1HTP', 'LY2ACC', 'LY2ACS', 'LY2ASC', 'LY2ASS', 'LY2ASTC', 'LY2ASTS', 'LY2ATGC', 'LY2ATGS', 'LY2ATP', 'LY2DivAT', 'LY2DivHT', 'LY2HCC', 'LY2HCS', 'LY2HSC', 'LY2HSS', 'LY2HSTC', 'LY2HSTS', 'LY2HTGC', 'LY2HTGS', 'LY2HTP', 'LY3ACC', 'LY3ACS', 'LY3ASC', 'LY3ASS', 'LY3ASTC', 'LY3ASTS', 'LY3ATGC', 'LY3ATGS', 'LY3ATP', 'LY3DivAT', 'LY3DivHT', 'LY3HCC', 'LY3HCS', 'LY3HSC', 'LY3HSS', 'LY3HSTC', 'LY3HSTS', 'LY3HTGC', 'LY3HTGS', 'LY3HTP', 'LY4ACC', 'LY4ACS', 'LY4ASC', 'LY4ASS', 'LY4ASTC', 'LY4ASTS', 'LY4ATGC', 'LY4ATGS', 'LY4ATP', 'LY4DivAT', 'LY4DivHT', 'LY4HCC', 'LY4HCS', 'LY4HSC', 'LY4HSS', 'LY4HSTC', 'LY4HSTS', 'LY4HTGC', 'LY4HTGS', 'LY4HTP', 'MW', 'B365H', 'B365D', 'B365A'] 357\n"
     ]
    }
   ],
   "source": [
    "# Separate into feature set and target variable\n",
    "#FTR = Full Time Result (H=Home Win, D=Draw, A=Away Win)\n",
    "train_set=data[pd.to_datetime(data.Date)<pd.to_datetime('{}-{}-{}'.format(train_val_split_date[0],train_val_split_date[1],train_val_split_date[2]))]\n",
    "validate_new_result_set=data[pd.to_datetime(data.Date)>pd.to_datetime('{}-{}-{}'.format(train_val_split_date[0],train_val_split_date[1],train_val_split_date[2]))]\n",
    "#validate_new_result_set=temp[pd.to_datetime(temp.Date)<pd.to_datetime('{}-{}-{}'.format(train_val_stop_date[0],train_val_stop_date[1],train_val_stop_date[2]))]\n",
    "\n",
    "print len(validate_new_result_set)\n",
    "#train_set=data\n",
    "#print list(train_set.columns)\n",
    "\n",
    "\n",
    "\n",
    "# Standardising the data.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Center to the mean and component wise scale to unit variance.\n",
    "cols = ['HTGS','HTGC','ATGS','ATGC','HTP','ATP','MW']#,\n",
    "            #'HSS', 'ASS', \n",
    "            #'HSC', 'ASC', 'HSTS', 'ASTS', 'HSTC','ASTC', \n",
    "            #'HCS', 'ACS', 'HCC', 'ACC'] \n",
    "\n",
    "for i in range(1,num_last_games+1):\n",
    "    cols.append('HM' + str(i)+'Scored')               \n",
    "    cols.append('AM' + str(i)+'Scored')\n",
    "    cols.append('HM' + str(i)+'Conceded')               \n",
    "    cols.append('AM' + str(i)+'Conceded')\n",
    "    cols.append('HM' + str(i)+'OppPoints')               \n",
    "    cols.append('AM' + str(i)+'OppPoints')\n",
    "    cols.append('HM' + str(i)+'OppScored')               \n",
    "    cols.append('AM' + str(i)+'OppScored')\n",
    "    cols.append('HM' + str(i)+'OppConceded')               \n",
    "    cols.append('AM' + str(i)+'OppConceded')\n",
    "\n",
    "for i in range(0,num_last_seasons-1):\n",
    "    #cols.append('LY' + str(i)+'ACC')               \n",
    "    #cols.append('LY' + str(i)+'HCC') \n",
    "    #cols.append('LY' + str(i)+'ACS')               \n",
    "    #cols.append('LY' + str(i)+'HCS') \n",
    "    #cols.append('LY' + str(i)+'ASC')               \n",
    "    #cols.append('LY' + str(i)+'HSC') \n",
    "    #cols.append('LY' + str(i)+'ASS')               \n",
    "    #cols.append('LY' + str(i)+'HSS') \n",
    "    #cols.append('LY' + str(i)+'ASTC')               \n",
    "    #cols.append('LY' + str(i)+'HSTC') \n",
    "    #cols.append('LY' + str(i)+'ASTS')               \n",
    "    #cols.append('LY' + str(i)+'HSTS')\n",
    "    cols.append('LY' + str(i)+'ATGC')               \n",
    "    cols.append('LY' + str(i)+'HTGC') \n",
    "    cols.append('LY' + str(i)+'ATGS')               \n",
    "    cols.append('LY' + str(i)+'HTGS') \n",
    "    cols.append('LY' + str(i)+'ATP')               \n",
    "    cols.append('LY' + str(i)+'HTP') \n",
    "    cols.append('LY' + str(i)+'DivAT')               \n",
    "    cols.append('LY' + str(i)+'DivHT')\n",
    "    \n",
    "for i in range(1,num_last_seasons*2-1):\n",
    "    cols.append('LMUP' + str(i)+'HT')               \n",
    "    cols.append('LMUP' + str(i)+'AT')    \n",
    "\n",
    "\n",
    "    \n",
    "#for col in cols:\n",
    "#    train_set = train_set[train_set[col].notnull()]\n",
    "\n",
    "\n",
    "X_all = train_set.drop(['FTR'],1)\n",
    "X_validate_new_result_set=validate_new_result_set.drop(['FTR'],1)\n",
    "y_all = train_set['FTR']\n",
    "y_validate_new_result_set=validate_new_result_set['FTR']\n",
    "\n",
    "scaler=StandardScaler()\n",
    "\n",
    "#for col in cols:\n",
    "    #print X_all.ix[~X_all[col].isnull()[col]]\n",
    "   # print len(X_all[~X_all[col].isnull()][col].as_matrix().reshape(-1,1))\n",
    "   # X_all.ix[~X_all[col].isnull(),col]= scaler.fit_transform(X_all[~X_all[col].isnull()][col].as_matrix().reshape(-1,1))\n",
    "   # X_validate_new_result_set.ix[~X_validate_new_result_set[col].isnull(),col]= scaler.fit_transform(X_validate_new_result_set[~X_validate_new_result_set[col].isnull()][col].as_matrix().reshape(-1,1))\n",
    "  \n",
    "    #print X_all[~X_all[col].isnull()][col]\n",
    "    \n",
    "betting_cols=['B365H','B365D','B365A']\n",
    "column=train_set['B365H']\n",
    "column.fillna(1.888,inplace=True)#2.3601\n",
    "train_set['B365H']=column\n",
    "cols.append('B365H')\n",
    "  \n",
    "column=train_set['B365D']\n",
    "column.fillna(1.888,inplace=True)#3.4961\n",
    "train_set['B365D']=column\n",
    "cols.append('B365D')\n",
    "\n",
    "column=train_set['B365A']\n",
    "column.fillna(1.888,inplace=True)#4.1027\n",
    "train_set['B365A']=column\n",
    "cols.append('B365A')\n",
    "    \n",
    "betting_cols=['B365H','B365D','B365A','Date','Div','HomeTeam','AwayTeam','MW']\n",
    "\n",
    "for col in betting_cols:\n",
    "    y_all=pd.concat([y_all,train_set[col]],axis=1)\n",
    "    y_validate_new_result_set=pd.concat([y_validate_new_result_set,validate_new_result_set[col]],axis=1)\n",
    "    \n",
    "print len(X_all),len(y_all)    \n",
    "print list(X_all.columns),len(X_all.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angloher/projects/Predicting_Winning_Teams/PredictEnv/lib/python2.7/site-packages/ipykernel_launcher.py:24: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [FTR, B365H, B365D, B365A, Date, Div, HomeTeam, AwayTeam, MW]\n",
      "Index: []\n",
      "160610\n",
      "['HTGS', 'HTGC', 'ATGS', 'ATGC', 'HTP', 'ATP', 'MW', 'HM1Scored', 'AM1Scored', 'HM1Conceded', 'AM1Conceded', 'HM1OppPoints', 'AM1OppPoints', 'HM1OppScored', 'AM1OppScored', 'HM1OppConceded', 'AM1OppConceded', 'HM2Scored', 'AM2Scored', 'HM2Conceded', 'AM2Conceded', 'HM2OppPoints', 'AM2OppPoints', 'HM2OppScored', 'AM2OppScored', 'HM2OppConceded', 'AM2OppConceded', 'HM3Scored', 'AM3Scored', 'HM3Conceded', 'AM3Conceded', 'HM3OppPoints', 'AM3OppPoints', 'HM3OppScored', 'AM3OppScored', 'HM3OppConceded', 'AM3OppConceded', 'HM4Scored', 'AM4Scored', 'HM4Conceded', 'AM4Conceded', 'HM4OppPoints', 'AM4OppPoints', 'HM4OppScored', 'AM4OppScored', 'HM4OppConceded', 'AM4OppConceded', 'HM5Scored', 'AM5Scored', 'HM5Conceded', 'AM5Conceded', 'HM5OppPoints', 'AM5OppPoints', 'HM5OppScored', 'AM5OppScored', 'HM5OppConceded', 'AM5OppConceded', 'HM6Scored', 'AM6Scored', 'HM6Conceded', 'AM6Conceded', 'HM6OppPoints', 'AM6OppPoints', 'HM6OppScored', 'AM6OppScored', 'HM6OppConceded', 'AM6OppConceded', 'HM7Scored', 'AM7Scored', 'HM7Conceded', 'AM7Conceded', 'HM7OppPoints', 'AM7OppPoints', 'HM7OppScored', 'AM7OppScored', 'HM7OppConceded', 'AM7OppConceded', 'HM8Scored', 'AM8Scored', 'HM8Conceded', 'AM8Conceded', 'HM8OppPoints', 'AM8OppPoints', 'HM8OppScored', 'AM8OppScored', 'HM8OppConceded', 'AM8OppConceded', 'HM9Scored', 'AM9Scored', 'HM9Conceded', 'AM9Conceded', 'HM9OppPoints', 'AM9OppPoints', 'HM9OppScored', 'AM9OppScored', 'HM9OppConceded', 'AM9OppConceded', 'HM10Scored', 'AM10Scored', 'HM10Conceded', 'AM10Conceded', 'HM10OppPoints', 'AM10OppPoints', 'HM10OppScored', 'AM10OppScored', 'HM10OppConceded', 'AM10OppConceded', 'HM11Scored', 'AM11Scored', 'HM11Conceded', 'AM11Conceded', 'HM11OppPoints', 'AM11OppPoints', 'HM11OppScored', 'AM11OppScored', 'HM11OppConceded', 'AM11OppConceded', 'HM12Scored', 'AM12Scored', 'HM12Conceded', 'AM12Conceded', 'HM12OppPoints', 'AM12OppPoints', 'HM12OppScored', 'AM12OppScored', 'HM12OppConceded', 'AM12OppConceded', 'HM13Scored', 'AM13Scored', 'HM13Conceded', 'AM13Conceded', 'HM13OppPoints', 'AM13OppPoints', 'HM13OppScored', 'AM13OppScored', 'HM13OppConceded', 'AM13OppConceded', 'HM14Scored', 'AM14Scored', 'HM14Conceded', 'AM14Conceded', 'HM14OppPoints', 'AM14OppPoints', 'HM14OppScored', 'AM14OppScored', 'HM14OppConceded', 'AM14OppConceded', 'HM15Scored', 'AM15Scored', 'HM15Conceded', 'AM15Conceded', 'HM15OppPoints', 'AM15OppPoints', 'HM15OppScored', 'AM15OppScored', 'HM15OppConceded', 'AM15OppConceded', 'HM16Scored', 'AM16Scored', 'HM16Conceded', 'AM16Conceded', 'HM16OppPoints', 'AM16OppPoints', 'HM16OppScored', 'AM16OppScored', 'HM16OppConceded', 'AM16OppConceded', 'HM17Scored', 'AM17Scored', 'HM17Conceded', 'AM17Conceded', 'HM17OppPoints', 'AM17OppPoints', 'HM17OppScored', 'AM17OppScored', 'HM17OppConceded', 'AM17OppConceded', 'HM18Scored', 'AM18Scored', 'HM18Conceded', 'AM18Conceded', 'HM18OppPoints', 'AM18OppPoints', 'HM18OppScored', 'AM18OppScored', 'HM18OppConceded', 'AM18OppConceded', 'HM19Scored', 'AM19Scored', 'HM19Conceded', 'AM19Conceded', 'HM19OppPoints', 'AM19OppPoints', 'HM19OppScored', 'AM19OppScored', 'HM19OppConceded', 'AM19OppConceded', 'HM20Scored', 'AM20Scored', 'HM20Conceded', 'AM20Conceded', 'HM20OppPoints', 'AM20OppPoints', 'HM20OppScored', 'AM20OppScored', 'HM20OppConceded', 'AM20OppConceded', 'LY0ATGC', 'LY0HTGC', 'LY0ATGS', 'LY0HTGS', 'LY0ATP', 'LY0HTP', 'LY0DivAT', 'LY0DivHT', 'LY1ATGC', 'LY1HTGC', 'LY1ATGS', 'LY1HTGS', 'LY1ATP', 'LY1HTP', 'LY1DivAT', 'LY1DivHT', 'LY2ATGC', 'LY2HTGC', 'LY2ATGS', 'LY2HTGS', 'LY2ATP', 'LY2HTP', 'LY2DivAT', 'LY2DivHT', 'LY3ATGC', 'LY3HTGC', 'LY3ATGS', 'LY3HTGS', 'LY3ATP', 'LY3HTP', 'LY3DivAT', 'LY3DivHT', 'LY4ATGC', 'LY4HTGC', 'LY4ATGS', 'LY4HTGS', 'LY4ATP', 'LY4HTP', 'LY4DivAT', 'LY4DivHT', 'LMUP1HT', 'LMUP1AT', 'LMUP2HT', 'LMUP2AT', 'LMUP3HT', 'LMUP3AT', 'LMUP4HT', 'LMUP4AT', 'LMUP5HT', 'LMUP5AT', 'LMUP6HT', 'LMUP6AT', 'LMUP7HT', 'LMUP7AT', 'LMUP8HT', 'LMUP8AT', 'LMUP9HT', 'LMUP9AT', 'LMUP10HT', 'LMUP10AT', 'B365H', 'B365D', 'B365A', 'league', 'E', 'D', 'F', 'SP', 'B', 'N', 'G', 'T', 'P', 'I'] 281\n",
      "{dtype('int64'): Index([u'MW', u'LY0DivAT', u'LY0DivHT', u'LY1DivAT', u'LY1DivHT', u'LY2DivAT',\n",
      "       u'LY2DivHT', u'LY3DivAT', u'LY3DivHT', u'LY4DivAT', u'LY4DivHT', u'E',\n",
      "       u'D', u'F', u'SP', u'B', u'N', u'G', u'T', u'P', u'I'],\n",
      "      dtype='object'), dtype('float64'): Index([u'HTGS', u'HTGC', u'ATGS', u'ATGC', u'HTP', u'ATP', u'HM1Scored',\n",
      "       u'AM1Scored', u'HM1Conceded', u'AM1Conceded',\n",
      "       ...\n",
      "       u'LMUP8HT', u'LMUP8AT', u'LMUP9HT', u'LMUP9AT', u'LMUP10HT',\n",
      "       u'LMUP10AT', u'B365H', u'B365D', u'B365A', u'league'],\n",
      "      dtype='object', length=260)}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "#last 5 games for both sides\n",
    "\n",
    "def test_letter(c):\n",
    "    if not c.isdigit():\n",
    "        return 4\n",
    "    else:\n",
    "        return c\n",
    "    \n",
    "def preprocess_features(X,cols):\n",
    "    ''' Preprocesses the football data and converts catagorical variables into dummy variables. '''\n",
    "    Divisions=['E0','E1','E2','E3','EC','D1','D2','F1','F2',\n",
    "               'SP1','SP2','B1','N1','G1','T1','P1','I1','I2']#,\n",
    "               #'Br1','Swe1','Ru1','No1','Ch1','SC0','SC1','SC2','SC3',\n",
    "               #'Au1','Fin1','Swi1','Den1','Jap1','Mex1','Po1','Ro1']\n",
    "    cols.append('league')\n",
    "    X['league']=None\n",
    "    for div in Divisions:\n",
    "        X[div[0:-1]]=0\n",
    "        if div[0:-1] not in cols:\n",
    "            cols.append(div[0:-1])\n",
    "    for div in Divisions:\n",
    "        X.ix[X['Div']==div,div[0:-1]]=1\n",
    "        X.ix[X['Div']==div,'league']=float(test_letter(div[-1]))\n",
    "    \n",
    "    return X[cols]\n",
    "\n",
    "val_cols=list(cols)\n",
    "X_all=preprocess_features(X_all.copy(),val_cols)\n",
    "val_cols=list(cols)\n",
    "X_validate_new_result_set=preprocess_features(X_validate_new_result_set.copy(),val_cols)\n",
    "#X_all=X_all[cols]\n",
    "\n",
    "X_all.fillna(0,inplace=True)\n",
    "X_validate_new_result_set.fillna(0,inplace=True)\n",
    "\n",
    "print y_all[y_all['FTR'].isnull()==True]\n",
    "\n",
    "\n",
    "\n",
    "print len(X_all)\n",
    "print list(X_all.columns),len(X_all.columns)\n",
    "\n",
    "print X_all.columns.to_series().groupby(X_all.dtypes).groups\n",
    "#start=time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature information by printing the first five rows\n",
    "\n",
    "print \"\\nFeature values:\"\n",
    "display(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length training data: 128488\n",
      "length test data: 16061\n",
      "length validation data: 16061\n",
      "length new test data 1330\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "#fill leagueName data\n",
    "\n",
    "\n",
    "# Shuffle and split the dataset into training and testing set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, \n",
    "                                                    test_size = len(y_all)/5,\n",
    "                                                    random_state = 2,\n",
    "                                                    stratify = y_all['FTR'])\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, \n",
    "                                                    test_size = len(y_test)/2,\n",
    "                                                    random_state = 2,\n",
    "                                                    stratify = y_test['FTR'])\n",
    "\n",
    "\n",
    "print 'length training data: {}\\nlength test data: {}\\nlength validation data: {}'.format(len(X_train),len(X_test),len(X_val))\n",
    "print 'length new test data {}'.format(len(X_validate_new_result_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_train_target(y_train):\n",
    "    matrix=y_train\n",
    "    i=0\n",
    "    #print matrix\n",
    "    for train in y_train:\n",
    "        #print train[0]\n",
    "        if train[0]=='H':\n",
    "            matrix[i][0]=0\n",
    "        elif train[0]=='D':\n",
    "            matrix[i][0]=1\n",
    "        else:\n",
    "            matrix[i][0]=2\n",
    "        i+=1\n",
    "    matrix=np.array(matrix,np.float)\n",
    "    return matrix\n",
    "\n",
    "def split_target(y_train):\n",
    "    matrix=np.array([0]*len(y_train),np.float)\n",
    "    i=0\n",
    "    #print matrix\n",
    "    for train in y_train:\n",
    "        #print train[0]\n",
    "        if train=='H':\n",
    "            matrix[i]=0\n",
    "        elif train=='D':\n",
    "            matrix[i]=1\n",
    "        else:\n",
    "            matrix[i]=2\n",
    "        i+=1\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def predict(data_loader,model):\n",
    "    probas=[]\n",
    "    for i,data in enumerate(data_loader,0):\n",
    "        inputs,labels =data\n",
    "        inputs,labels=Variable(inputs),Variable(labels)\n",
    "        pred=model(inputs)\n",
    "        probas.append(pred.data.numpy())\n",
    "        \n",
    "    return probas\n",
    "\n",
    "def accuracy(data_loader,model):\n",
    "    summe=0\n",
    "    count=0\n",
    "    for i,data in enumerate(data_loader,0):\n",
    "        inputs,labels =data\n",
    "        inputs,labels=Variable(inputs),Variable(labels)\n",
    "        pred=model(inputs)\n",
    "        #print pred\n",
    "        _,predicted=torch.max(pred.data,1)\n",
    "        predicted=predicted.numpy()\n",
    "        labels=labels.data.numpy()\n",
    "        for i in range(len(predicted)):\n",
    "            if labels[i]==predicted[i]:\n",
    "                summe+=1# labels[i][predicted[i]]\n",
    "        \n",
    "        count+=len(predicted)\n",
    "    \n",
    "    return float(summe)/float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "def decide_on_bet(probas,odds,result,thresh,league,tax,verbose,alpha,list_of_bets,list_of_data):\n",
    "    exp_ret_home=probas[0]*odds[0]\n",
    "    exp_ret_draw=probas[1]*odds[1]\n",
    "    exp_ret_away=probas[2]*odds[2]\n",
    "    bet=1\n",
    "    win=0\n",
    "    loss=0\n",
    "    bet_count=0\n",
    "    bet_amount=0\n",
    "    if odds[0]==1.888:#2.3601\n",
    "        pass\n",
    "    else:\n",
    "        #print exp_ret_home,exp_ret_draw,exp_ret_away\n",
    "        if exp_ret_home>thresh and exp_ret_home > exp_ret_draw and exp_ret_home > exp_ret_away:\n",
    "            if verbose:\n",
    "                print 'In {} {} vs {} I bet {} euros on Home with odds {} and expected return {}'.format(odds[4],odds[5],odds[6],(exp_ret_home-thresh)*alpha,odds[0],exp_ret_home)\n",
    "            bet_count+=1\n",
    "            bet=((exp_ret_home-thresh)*alpha)\n",
    "            bet_amount+=bet\n",
    "            list_of_data.append([probas[0],odds[3],odds[4],bet,odds[5],odds[6],odds[7]])\n",
    "            if result=='H':\n",
    "                if verbose:\n",
    "                    print 'and won {}'.format(bet*odds[0]*(1-tax)-bet)\n",
    "                win+=bet*odds[0]*(1-tax)-bet\n",
    "                list_of_bets.append(bet*odds[0]*(1-tax)-bet)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print 'and lost {}'.format(-bet)\n",
    "                loss+=bet\n",
    "                list_of_bets.append(-bet)\n",
    "\n",
    "        if exp_ret_draw>thresh and exp_ret_draw > exp_ret_home and exp_ret_draw > exp_ret_away:\n",
    "            if verbose:\n",
    "                print 'In {} {} vs {} I bet {} euros on Draw with odds {} and expected return {}'.format(odds[4],odds[5],odds[6],(exp_ret_draw-thresh)*alpha,odds[1],exp_ret_draw)\n",
    "            bet_count+=1\n",
    "            bet=((exp_ret_draw-thresh)*alpha)\n",
    "            bet_amount+=bet\n",
    "            list_of_data.append([probas[1],odds[3],odds[4],bet,odds[5],odds[6],odds[7]])\n",
    "            if result=='D':\n",
    "                if verbose:\n",
    "                    print 'and won {}'.format(bet*odds[1]*(1-tax)-bet)\n",
    "                win+=bet*odds[1]*(1-tax)-bet\n",
    "                list_of_bets.append(bet*odds[1]*(1-tax)-bet)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print 'and lost {}'.format(-bet)\n",
    "                loss+=bet\n",
    "                list_of_bets.append(-bet)\n",
    "\n",
    "        if exp_ret_away>thresh and exp_ret_away > exp_ret_draw and exp_ret_away > exp_ret_home:\n",
    "            if verbose:\n",
    "                print 'In {} {} vs {} I bet {} euros on Away with odds {} and expected return {}'.format(odds[4],odds[5],odds[6],alpha*(exp_ret_away-thresh),odds[2],exp_ret_away)\n",
    "            bet_count+=1\n",
    "            bet=((exp_ret_away-thresh)*alpha)\n",
    "            bet_amount+=bet\n",
    "            list_of_data.append([probas[2],odds[3],odds[4],bet,odds[5],odds[6],odds[7]])\n",
    "            if result=='A':\n",
    "                if verbose:\n",
    "                    print 'and won {}'.format(bet*odds[2]*(1-tax)-bet)\n",
    "                win+=bet*odds[2]*(1-tax)-bet\n",
    "                list_of_bets.append(bet*odds[2]*(1-tax)-bet)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print 'and lost {}'.format(-bet)\n",
    "                loss+=bet\n",
    "                list_of_bets.append(-bet)\n",
    "    return win-loss,bet_count,bet_amount,list_of_bets,list_of_data\n",
    "\n",
    "\n",
    "def eval_model(model,val_loader,y_val):\n",
    "    \n",
    "\n",
    "    betting_data=y_val.drop(['FTR'],1).as_matrix()\n",
    "    result_data=y_val['FTR'].as_matrix()\n",
    "\n",
    "    threshs=[1]#np.linspace(0.5,2,10)\n",
    "    alphas=[1]\n",
    "    betting_tax=0\n",
    "    verbose=False\n",
    "\n",
    "\n",
    "    #best_model=torch.load('models/NN_63198_0.492167473654_0.473749169278.pt')\n",
    "\n",
    "    probas=predict(val_loader,model)[0]\n",
    "    #print probas[-100:]\n",
    "    for thresh in threshs:\n",
    "        #ret=[]\n",
    "        for alpha in alphas:\n",
    "            data_count=0\n",
    "            won_overall=0\n",
    "            bets_overall=0\n",
    "            amount_overall=0\n",
    "            list_of_bets=[]\n",
    "            list_of_data=[]\n",
    "            for i in range(len(betting_data)):\n",
    "                won,num_bets,amount,list_of_bets,list_of_data=decide_on_bet(probas[i],betting_data[i],result_data[i],thresh,league,betting_tax,verbose,alpha,list_of_bets,list_of_data)\n",
    "                amount_overall+=amount\n",
    "                won_overall+=won\n",
    "                bets_overall+=num_bets\n",
    "    if amount_overall==0:\n",
    "        return -10,-1\n",
    "    else:\n",
    "        return won_overall/float(amount_overall),bets_overall\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def make_sparse(array):\n",
    "    array=np.array(array.as_matrix(),dtype=np.float)\n",
    "    #print array.shape\n",
    "    v= torch.FloatTensor(array[~np.isnan(array)])\n",
    "    #v= torch.FloatTensor([x for x in array if not np.isnan(x)])\n",
    "    \n",
    "    #print np.argwhere(np.invert(np.isnan(array)))\n",
    "    i= torch.LongTensor(np.transpose(np.argwhere(np.invert(np.isnan(array)))))\n",
    "    #print i\n",
    "    #print v\n",
    "    return torch.sparse.FloatTensor(i,v)\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(X,y,bet_data):\n",
    "    could_have_won=bet_data.gather(1,y.view(-1,1))-1\n",
    "    #print could_have_won\n",
    "    bet= (torch.mul(X,bet_data)-1).clamp(0)\n",
    "    #print bet\n",
    "    won=bet.gather(1,y.view(-1,1))\n",
    "    #print won\n",
    "    loss= torch.sum(bet,dim=1).view(len(bet_data),1).clamp(0.05)#.sub_(won) 0.1\n",
    "    #print loss\n",
    "    won=torch.mul(won,bet_data.gather(1,y.view(-1,1)))\n",
    "    #print won\n",
    "    won=won-loss\n",
    "    #print won\n",
    "    loss=could_have_won-won\n",
    "    #print loss\n",
    "    loss=loss.clamp(0)\n",
    "    loss=torch.sum(loss)\n",
    "    could_have_won=torch.sum(could_have_won)\n",
    "    loss= loss.div(could_have_won)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "batch_sizes=[8000,9000,10000,12000]\n",
    "input_dim=len(X_train.columns)\n",
    "Hidden_dims=[int(input_dim/2),int(input_dim)]#,\n",
    "output_dim=3\n",
    "\n",
    "epochs=30\n",
    "\n",
    "models=[]\n",
    "\n",
    "#y_train.fillna(2,inplace=True)\n",
    "X_val.fillna(0,inplace=True)\n",
    "X_test.fillna(0,inplace=True)\n",
    "#print torch.from_numpy(X_val.as_matrix()).float()\n",
    "val_set=torch.utils.data.TensorDataset(torch.from_numpy(X_val.as_matrix()).float(),torch.from_numpy(split_target(y_val['FTR'])).long())\n",
    "val_loader=torch.utils.data.DataLoader(val_set,batch_size=len(X_val),shuffle=False,num_workers=8)\n",
    "\n",
    "test_set=torch.utils.data.TensorDataset(torch.from_numpy(X_test.as_matrix()).float(),torch.from_numpy(split_target(y_test['FTR'])).long())\n",
    "test_loader=torch.utils.data.DataLoader(test_set,batch_size=len(X_test),shuffle=False,num_workers=8)\n",
    "\n",
    "for Hidden_dim in Hidden_dims:\n",
    "    \n",
    "        \n",
    "    models.append(torch.nn.Sequential(\n",
    "        torch.nn.BatchNorm1d(input_dim),\n",
    "        torch.nn.Linear(input_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,2*Hidden_dim),\n",
    "        torch.nn.Linear(2*Hidden_dim,3*Hidden_dim),\n",
    "        torch.nn.Dropout(),\n",
    "        torch.nn.Linear(Hidden_dim*3,2*Hidden_dim),\n",
    "        torch.nn.Linear(2*Hidden_dim,Hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(Hidden_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,output_dim),\n",
    "        torch.nn.Sigmoid()))\n",
    "    \n",
    "    models.append(torch.nn.Sequential(\n",
    "        torch.nn.BatchNorm1d(input_dim),\n",
    "        torch.nn.Linear(input_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,Hidden_dim),\n",
    "        torch.nn.Dropout(),\n",
    "        torch.nn.Linear(Hidden_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,Hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(Hidden_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,output_dim),\n",
    "        torch.nn.Sigmoid()))\n",
    "    \n",
    "    models.append(torch.nn.Sequential(\n",
    "        torch.nn.BatchNorm1d(input_dim),\n",
    "        torch.nn.Linear(input_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,Hidden_dim),\n",
    "        torch.nn.Dropout(),\n",
    "        torch.nn.Linear(Hidden_dim,2*Hidden_dim),\n",
    "        torch.nn.Linear(2*Hidden_dim,3*Hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(3*Hidden_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,output_dim),\n",
    "        torch.nn.Sigmoid()))\n",
    "    '''   \n",
    "    models.append(torch.nn.Sequential(\n",
    "        torch.nn.BatchNorm1d(input_dim),\n",
    "        torch.nn.Linear(input_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,2*Hidden_dim),\n",
    "        torch.nn.Linear(2*Hidden_dim,3*Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim*3,2*Hidden_dim),\n",
    "        torch.nn.Linear(2*Hidden_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,output_dim),\n",
    "        torch.nn.Sigmoid()))\n",
    "    \n",
    "    models.append(torch.nn.Sequential(\n",
    "        torch.nn.BatchNorm1d(input_dim),\n",
    "        torch.nn.Linear(input_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,2*Hidden_dim),\n",
    "        torch.nn.Dropout(p=0.2),\n",
    "        torch.nn.Linear(Hidden_dim*2,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,Hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(Hidden_dim,Hidden_dim),\n",
    "        torch.nn.Linear(Hidden_dim,output_dim),\n",
    "        torch.nn.Sigmoid()))\n",
    "    '''\n",
    "'''    \n",
    "b_m=torch.load(\"/home/angloher/projects/Predicting_Winning_Teams/models/current_set/best_model.pt\")\n",
    "\n",
    "pretrained_dict= b_m.state_dict()\n",
    "for model in models:\n",
    "    model_dict=model.state_dict()\n",
    "    for i in range(len(pretrained_dict.items())):\n",
    "        #print model_dict.items()[i]\n",
    "        if len(model_dict.items()[i][1].shape)==2:\n",
    "            print pretrained_dict.items()[i][1].shape[0]\n",
    "            model_dict.items()[i][1][0:pretrained_dict.items()[i][1].shape[0],0:pretrained_dict.items()[i][1].shape[1]]=pretrained_dict.items()[i][1]\n",
    "        else:\n",
    "            model_dict.items()[i][1][0:len(pretrained_dict.items()[i][1])]=pretrained_dict.items()[i][1]\n",
    "        #print model_dict.items()[i]\n",
    "    model_dict.update(model_dict)\n",
    "    model.load_state_dict(model_dict,strict=False)\n",
    "#print pretrained_dict.items()[0]\n",
    "'''\n",
    "print len(models)\n",
    "for batch_size in batch_sizes:\n",
    "    #sparse_X_tensor=make_sparse(X_train)\n",
    "    #print sparse_X_tensor.shape[0]\n",
    "    #y_tensor=torch.from_numpy(split_train_target(y_train[['FTR','B365H','B365D','B365A']].as_matrix()))\n",
    "    #print y_tensor.shape[0]\n",
    "    train_set=torch.utils.data.TensorDataset(torch.from_numpy(X_train.as_matrix()).float(),torch.from_numpy(split_train_target(y_train[['FTR','B365H','B365D','B365A']].as_matrix())))\n",
    "    train_loader=torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=True,num_workers=8)\n",
    "    \n",
    "    for model in models:\n",
    "        #loss_fn=torch.nn.CrossEntropyLoss()\n",
    "        learning_rates=[5e-3,1e-3,1e-4]\n",
    "        for learning_rate in learning_rates:\n",
    "            print model,learning_rate,batch_size\n",
    "            #optimizer = torch.optim.SparseAdam(model.parameters(),lr=learning_rate)\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,weight_decay=0.001)\n",
    "            #scheduler=torch.optim.lr_scheduler.StepLR(optimizer,5)\n",
    "            test_acc=[0]\n",
    "            best_test_return=-1\n",
    "            best_index=0\n",
    "            train_acc=[0]\n",
    "            time=[0]\n",
    "            counter=0\n",
    "            t=0\n",
    "            while counter<epochs:\n",
    "                total_loss=0\n",
    "                #scheduler.step()\n",
    "                for i,data in enumerate(train_loader):\n",
    "                    #print data\n",
    "                    #inputs=sparse_X_tensor\n",
    "                    #labels=y_tensor\n",
    "                    inputs,labels =data\n",
    "                    bet_data= labels.numpy()\n",
    "                    bet_data=[x[1:] for x in bet_data]\n",
    "                    #bet_data=np.array(labels.numpy(),np.float())\n",
    "                    labels=labels.gather(1,torch.zeros(len(labels),1).long()).long()\n",
    "                    \n",
    "                    \n",
    "                    inputs,labels=Variable(inputs),Variable(labels)\n",
    "                    model.train()\n",
    "                    y_pred=model(inputs)\n",
    "                    #print y_pred\n",
    "                    #bet_data=y_train[i*batch_size:min((i+1)*batch_size,len(y_train))]\n",
    "                    #bet_data.fillna(0,inplace=True)\n",
    "                    #print bet_data\n",
    "                    bet_data=Variable(torch.from_numpy(np.array(bet_data,np.float)).float(),requires_grad=True)\n",
    "                    #print bet_data\n",
    "                    loss=custom_loss(y_pred,labels,bet_data)\n",
    "\n",
    "                    #loss=loss_fn(y_pred,labels)\n",
    "                    #print loss\n",
    "                    total_loss+=loss.data[0]\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    optimizer.step()\n",
    "                print (t,total_loss,counter)\n",
    "                t+=1\n",
    "                if (t%1)==0:\n",
    "                    model.eval()\n",
    "                    #train_acc.append(accuracy(train_loader,model))\n",
    "                    current_acc=accuracy(test_loader,model)\n",
    "                    current_return,bets_overall=eval_model(model,test_loader,y_test)\n",
    "                    print current_acc,current_return,bets_overall/float(len(y_test))\n",
    "                    counter+=1\n",
    "\n",
    "                    if current_return>best_test_return:\n",
    "                        best_test_return=current_return\n",
    "                        best_model=copy.deepcopy(model)\n",
    "                        best_t=t\n",
    "                        best_index=len(test_acc)\n",
    "                        counter=0\n",
    "                    test_acc.append(current_acc)\n",
    "                    time.append(t)\n",
    "\n",
    "            best_model.eval()\n",
    "            plt.plot(time,test_acc)\n",
    "            #plt.plot(time,train_acc)\n",
    "            plt.show()\n",
    "            print best_test_return\n",
    "            print 'best model at {} iterations with test accuracy: {}\\nvalid accuracy: {} '.format(best_index,test_acc[best_index],accuracy(val_loader,best_model))\n",
    "\n",
    "            torch.save(best_model, \"/home/angloher/projects/Predicting_Winning_Teams/models/NN_\"+str(best_test_return)+\"_\"+str(bets_overall/float(len(y_test)))+\".pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Sequential(\n",
      "  (0): BatchNorm1d(281, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (1): Linear(in_features=281, out_features=281)\n",
      "  (2): Linear(in_features=281, out_features=562)\n",
      "  (3): Linear(in_features=562, out_features=843)\n",
      "  (4): Dropout(p=0.5)\n",
      "  (5): Linear(in_features=843, out_features=562)\n",
      "  (6): Linear(in_features=562, out_features=281)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=281, out_features=281)\n",
      "  (9): Linear(in_features=281, out_features=3)\n",
      "  (10): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angloher/projects/Predicting_Winning_Teams/PredictEnv/local/lib/python2.7/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/angloher/projects/Predicting_Winning_Teams/PredictEnv/local/lib/python2.7/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/angloher/projects/Predicting_Winning_Teams/PredictEnv/local/lib/python2.7/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/angloher/projects/Predicting_Winning_Teams/PredictEnv/local/lib/python2.7/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/angloher/projects/Predicting_Winning_Teams/PredictEnv/local/lib/python2.7/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/angloher/projects/Predicting_Winning_Teams/PredictEnv/local/lib/python2.7/site-packages/torch/serialization.py:316: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Sigmoid' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import glob,os\n",
    "#prediction\n",
    "val_set=torch.utils.data.TensorDataset(torch.from_numpy(X_train.as_matrix()).float(),torch.from_numpy(split_target(y_train['FTR'])).float())\n",
    "val_loader=torch.utils.data.DataLoader(val_set,batch_size=len(X_train),shuffle=False,num_workers=8)\n",
    "\n",
    "best_models=[]\n",
    "for file in glob.glob(\"/home/angloher/projects/Predicting_Winning_Teams/models/current_set/best*.pt\"):\n",
    "    best_models.append(torch.load(file))#best_model2\n",
    "print len(best_models)\n",
    "#best_model3\n",
    "betting_data=y_train.drop(['FTR'],1).as_matrix()\n",
    "result_data=y_train['FTR'].as_matrix()\n",
    "\n",
    "\n",
    "threshs=[1]#np.linspace(0.5,2,10)\n",
    "alphas=[1]\n",
    "betting_tax=0\n",
    "verbose=False\n",
    "\n",
    "\n",
    "probas=np.zeros((len(betting_data),3),np.float)    \n",
    "\n",
    "\n",
    "for best_model in best_models:\n",
    "    print best_model\n",
    "    best_model.eval()\n",
    "    probas+=predict(val_loader,best_model)[0]\n",
    "    #print probas\n",
    "    \n",
    "probas/=float(len(best_models))\n",
    "#print probas[-100:]\n",
    "for thresh in threshs:\n",
    "    #ret=[]\n",
    "    for alpha in alphas:\n",
    "        data_count=0\n",
    "        won_overall=0\n",
    "        bets_overall=0\n",
    "        amount_overall=0\n",
    "        list_of_bets=[]\n",
    "        list_of_data=[]\n",
    "        for i in range(len(betting_data)):\n",
    "            won,num_bets,amount,list_of_bets,list_of_data=decide_on_bet(probas[i],betting_data[i],result_data[i],thresh,league,betting_tax,verbose,alpha,list_of_bets,list_of_data)\n",
    "            amount_overall+=amount\n",
    "            won_overall+=won\n",
    "            bets_overall+=num_bets\n",
    "\n",
    "            #ret.append(won_overall/float(amount_overall))\n",
    "        print 'average return on bets with overall is {}'.format(won_overall/float(amount_overall))\n",
    "        print 'won overall {} in {} bets with overall bet amount {}'.format(won_overall,bets_overall,amount_overall)\n",
    "        print ''\n",
    "        #plt.plot(alphas,ret,label=str(thresh))\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "#print list_of_data,betting_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "import scipy.stats\n",
    "\n",
    "num_bets_per_week=100\n",
    "\n",
    "dist=np.array(list(list_of_bets)).astype(np.float).reshape(len(list_of_bets),1)\n",
    "dist= dist/np.array(list_of_data)[:,3:4].astype(np.float)\n",
    "mean=np.average(dist,weights=np.array(list_of_data)[:,3:4].astype(np.float))\n",
    "std=math.sqrt(np.average((dist-mean)**2,weights=np.array(list_of_data)[:,3:4].astype(np.float)))\n",
    "std=std/np.sqrt(num_bets_per_week)\n",
    "print 'mean: {}, std: {}'.format(mean,std)\n",
    "print 'doubling rate: {} weeks'.format(0.7/mean)\n",
    "x=np.linspace(mean-3*std,mean+3*std,100)\n",
    "plt.plot(x,mlab.normpdf(x,mean,std))\n",
    "plt.show()\n",
    "print 'probability of losing money: {}'.format(scipy.stats.norm(mean,std).cdf(0))\n",
    "print 'probability of losing half the money: {}'.format(scipy.stats.norm(mean,std).cdf(-0.5))\n",
    "print 'probability of losing all the money: {}'.format(scipy.stats.norm(mean,std).cdf(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=np.array(list(list_of_bets)).astype(np.float).reshape(len(list_of_bets),1)\n",
    "#print dist\n",
    "num_bets=60\n",
    "for t in range(1000):\n",
    "    won=0\n",
    "    for i in range(num_bets):\n",
    "        won+=dist[np.random.randint(0,len(dist))]\n",
    "    print won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "proba_auswertung=[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "probas_dist=np.linspace(0,95,20)\n",
    "high_certainty=[]\n",
    "\n",
    "year_auswertung=[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "year_dist=np.linspace(1993,2018,26)\n",
    "\n",
    "runde_auswertung=[[],[]]\n",
    "\n",
    "Spieltag_auswertung=[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "Spieltag_dist=np.linspace(0,50,51)\n",
    "\n",
    "div_auswertung=[[],[],[],[],[],[],[],[],[],[]]\n",
    "Divisions=['B','D','E','F','G','I','N','P','S','T']\n",
    "\n",
    "print len(year_auswertung)\n",
    "print year_dist\n",
    "#print list_of_data\n",
    "\n",
    "for i in range(len(list_of_bets)):\n",
    "    proba_auswertung[int(list_of_data[i][0]*100/5)-1].append(list_of_bets[i])\n",
    "    if list_of_data[i][0]*100>99:\n",
    "        high_certainty.append(list_of_bets[i])\n",
    "    \n",
    "    #print int(re.split('-',list_of_data[i][1])[0])-1993\n",
    "    year_auswertung[int(re.split('-',list_of_data[i][1])[0])-1993].append(list_of_bets[i])\n",
    "    if int(re.split('-',list_of_data[i][1])[1])==3:\n",
    "        runde_auswertung[1].append(list_of_bets[i])\n",
    "    else:\n",
    "        runde_auswertung[0].append(list_of_bets[i])\n",
    "    \n",
    "    #print list_of_data[i]\n",
    "    #print list_of_data[i][6]\n",
    "    #print Spieltag_auswertung\n",
    "    Spieltag_auswertung[list_of_data[i][6]].append(list_of_bets[i])\n",
    "    \n",
    "    for j in range(len(Divisions)):\n",
    "        if list_of_data[i][2][0]==Divisions[j]:\n",
    "            div_auswertung[j].append(list_of_bets[i])\n",
    "\n",
    "for i in range(len(proba_auswertung)):\n",
    "    proba_auswertung[i]=np.mean(proba_auswertung[i])\n",
    "    \n",
    "for i in range(len(year_auswertung)):\n",
    "    year_auswertung[i]=np.mean(year_auswertung[i])\n",
    "\n",
    "print Spieltag_auswertung[1]\n",
    "for i in range(len(Spieltag_auswertung)):\n",
    "    Spieltag_auswertung[i]=np.mean(Spieltag_auswertung[i])    \n",
    "    \n",
    "for i in range(len(div_auswertung)):\n",
    "    div_auswertung[i]=np.mean(div_auswertung[i])\n",
    "\n",
    "print np.mean(high_certainty)    \n",
    "plt.plot(probas_dist,proba_auswertung)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(year_dist,year_auswertung)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(Divisions,div_auswertung)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(Spieltag_dist,Spieltag_auswertung)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "print np.mean(runde_auswertung[0]),np.mean(runde_auswertung[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print X_validate_new_result_set\n",
    "val_set=torch.utils.data.TensorDataset(torch.from_numpy(X_validate_new_result_set.as_matrix()).float(),torch.from_numpy(split_target(y_validate_new_result_set['FTR'])).float())\n",
    "val_loader=torch.utils.data.DataLoader(val_set,batch_size=len(X_validate_new_result_set),shuffle=False,num_workers=8)\n",
    "#best_model=torch.load(\"/home/angloher/projects/Predicting_Winning_Teams/models/best_model.pt\")\n",
    "\n",
    "betting_data=y_validate_new_result_set.drop(['FTR'],1).as_matrix()\n",
    "result_data=y_validate_new_result_set['FTR'].as_matrix()\n",
    "\n",
    "threshs=[1]#np.linspace(0.5,2,10)\n",
    "alphas=[1]\n",
    "betting_tax=0\n",
    "verbose=True\n",
    "\n",
    "    \n",
    "#best_model=torch.load('models/NN_63198_0.492167473654_0.473749169278.pt')\n",
    "\n",
    "probas=np.zeros((len(betting_data),3),np.float)   \n",
    "print betting_data\n",
    "for best_model in best_models:\n",
    "    best_model.eval()\n",
    "    probas+=predict(val_loader,best_model)[0]\n",
    "    #print probas\n",
    "    \n",
    "probas/=float(len(best_models))\n",
    "\n",
    "print len(probas)\n",
    "for thresh in threshs:\n",
    "    #ret=[]\n",
    "    for alpha in alphas:\n",
    "        data_count=0\n",
    "        won_overall=0\n",
    "        bets_overall=0\n",
    "        amount_overall=0\n",
    "        list_of_bets=[]\n",
    "        list_of_data=[]\n",
    "        for i in range(len(betting_data)):\n",
    "            won,num_bets,amount,list_of_bets,list_of_data=decide_on_bet(probas[i],betting_data[i],result_data[i],thresh,league,betting_tax,verbose,alpha,list_of_bets,list_of_data)\n",
    "            amount_overall+=amount\n",
    "            won_overall+=won\n",
    "            bets_overall+=num_bets\n",
    "\n",
    "            #ret.append(won_overall/float(amount_overall))\n",
    "        print 'average return on bets with overall is {}'.format(won_overall/float(amount_overall))\n",
    "        print 'won overall {} in {} bets with overall bet amount {}'.format(won_overall,bets_overall,amount_overall)\n",
    "        print ''\n",
    "        #plt.plot(alphas,ret,label=str(thresh))\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "#print list_of_data,betting_data\n",
    "betrate=bets_overall/float(len(probas))\n",
    "print betrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "weeks_auswertung=[]\n",
    "weeks=[]\n",
    "week_counter=[]\n",
    "weeks_data=[]\n",
    "\n",
    "stop_date=datetime.date.today()\n",
    "\n",
    "\n",
    "#start_date=datetime.datetime.strptime(''.join(train_val_split_date),'%m%d%Y').date()\n",
    "start_date=datetime.datetime.strptime('09-02-2018','%d-%m-%Y').date()\n",
    " \n",
    "week_count=0\n",
    "while start_date+week_count*datetime.timedelta(4)<stop_date:\n",
    "    weeks.append(start_date+week_count*datetime.timedelta(4))\n",
    "    weeks_auswertung.append([])\n",
    "    week_counter.append([])\n",
    "    weeks_data.append([])\n",
    "    week_count+=1\n",
    "    \n",
    "for i in range(len(list_of_bets)):\n",
    "    for j in range(len(weeks)):    \n",
    "        if datetime.datetime.strptime(list_of_data[i][1],'%Y-%m-%d').date()>=weeks[j]and datetime.datetime.strptime(list_of_data[i][1],'%Y-%m-%d').date()<weeks[j+1]:\n",
    "            weeks_auswertung[j].append(list_of_bets[i])\n",
    "            weeks_data[j].append(list_of_data[i])\n",
    "\n",
    "lost=0\n",
    "won=0\n",
    "summe=0\n",
    "einsatz=0\n",
    "count=0\n",
    "w=0\n",
    "for c in range(len(weeks_auswertung[w])):\n",
    "    #print c,weeks_auswertung[0][c],weeks_data[0][c]\n",
    "    \n",
    "    if weeks_data[w][c][3]*4>0.8:\n",
    "        count+=1\n",
    "        einsatz+=weeks_data[w][c][3]\n",
    "        if weeks_auswertung[w][c]*weeks_data[w][c][3]>0.5:\n",
    "            print weeks_data[w][c],weeks_auswertung[w][c]\n",
    "            summe+=weeks_auswertung[w][c]*weeks_data[w][c][3]\n",
    "        if weeks_auswertung[w][c]>0:    \n",
    "            won+=(1+weeks_auswertung[w][c])*weeks_data[w][c][3]\n",
    "\n",
    "print count\n",
    "print einsatz*4        \n",
    "print won*4\n",
    "print summe/won\n",
    "print won/float(len(weeks_auswertung[0]))\n",
    "\n",
    "for i in range(len(weeks_auswertung)):\n",
    "    week_counter[i]=len(weeks_auswertung[i])\n",
    "    weeks_auswertung[i]=np.mean(weeks_auswertung[i])\n",
    "    if np.mean(weeks_auswertung[i])<0:\n",
    "        lost+=1\n",
    "        \n",
    "print lost/float(len(weeks))\n",
    "print weeks_auswertung\n",
    "print week_counter\n",
    "print weeks\n",
    "\n",
    "fig=plt.figure(figsize=(20,20))\n",
    "print np.mean(weeks_auswertung[:-1])\n",
    "print np.std(weeks_auswertung[:-1])\n",
    "ax=fig.add_subplot(111)\n",
    "ax.plot(weeks,weeks_auswertung)\n",
    "\n",
    "fig.savefig('test.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_bets_per_week=150*betrate\n",
    "\n",
    "dist=np.array(list(list_of_bets)).astype(np.float).reshape(len(list_of_bets),1)\n",
    "dist= dist/np.array(list_of_data)[:,3:4].astype(np.float)\n",
    "mean=np.average(dist,weights=np.array(list_of_data)[:,3:4].astype(np.float))\n",
    "std=math.sqrt(np.average((dist-mean)**2,weights=np.array(list_of_data)[:,3:4].astype(np.float)))\n",
    "std=std/np.sqrt(num_bets_per_week)\n",
    "print 'mean: {}, std: {}'.format(mean,std)\n",
    "print 'doubling rate: {} weeks'.format(0.7/mean)\n",
    "x=np.linspace(mean-3*std,mean+3*std,100)\n",
    "plt.plot(x,mlab.normpdf(x,mean,std))\n",
    "plt.show()\n",
    "print 'probability of losing money: {}'.format(scipy.stats.norm(mean,std).cdf(0))\n",
    "print 'probability of losing half the money: {}'.format(scipy.stats.norm(mean,std).cdf(-0.5))\n",
    "print 'probability of losing all the money: {}'.format(scipy.stats.norm(mean,std).cdf(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "test_set=torch.utils.data.TensorDataset(torch.from_numpy(X_test.as_matrix()).float(),torch.from_numpy(split_target(y_test['FTR'])).float())\n",
    "test_loader=torch.utils.data.DataLoader(test_set,batch_size=len(X_test),shuffle=False,num_workers=8)\n",
    "\n",
    "betting_data=y_test.drop(['FTR'],1).as_matrix()\n",
    "result_data=y_test['FTR'].as_matrix()\n",
    "\n",
    "threshs=[1]#np.linspace(0.5,2,10)\n",
    "alphas=[1]#np.linspace(-5,5,20)\n",
    "betting_tax=0\n",
    "verbose=False\n",
    "\n",
    "\n",
    "probas=np.zeros((len(betting_data),3),np.float)   \n",
    "\n",
    "for best_model in best_models:\n",
    "    best_model.eval()\n",
    "    probas+=predict(test_loader,best_model)[0]\n",
    "    #print probas\n",
    "    \n",
    "probas/=float(len(best_models))\n",
    "\n",
    "for thresh in threshs:\n",
    "        #ret=[]\n",
    "    for alpha in alphas:\n",
    "        print thresh,alpha\n",
    "        data_count=0\n",
    "        won_overall=0\n",
    "        bets_overall=0\n",
    "        amount_overall=0\n",
    "        list_of_bets=[]\n",
    "        list_of_data=[]\n",
    "        for i in range(len(betting_data)):\n",
    "            won,num_bets,amount,list_of_bets,list_of_data=decide_on_bet(probas[i],betting_data[i],result_data[i],thresh,league,betting_tax,verbose,alpha,list_of_bets,list_of_data)\n",
    "            amount_overall+=amount\n",
    "            won_overall+=won\n",
    "            bets_overall+=num_bets\n",
    "\n",
    "            #ret.append(won_overall/float(amount_overall))\n",
    "        print 'average return on bets with overall is {}'.format(won_overall/float(amount_overall))\n",
    "        print 'won overall {} in {} bets'.format(won_overall,bets_overall)\n",
    "        print ''\n",
    "        print won_overall\n",
    "        print sum(list_of_bets)\n",
    "        print amount_overall\n",
    "        print sum(np.array(list_of_data)[:,3:4].astype(np.float))\n",
    "       # plt.plot(alphas,ret,label=str(thresh))\n",
    "    #plt.legend()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "import scipy.stats\n",
    "\n",
    "num_bets_per_week=50\n",
    "#print np.array(list_of_data)[:,3:4]\n",
    "dist=np.array(list(list_of_bets)).astype(np.float).reshape(len(list_of_bets),1)\n",
    "dist= dist/np.array(list_of_data)[:,3:4].astype(np.float)\n",
    "mean=np.average(dist,weights=np.array(list_of_data)[:,3:4].astype(np.float))\n",
    "std=math.sqrt(np.average((dist-mean)**2,weights=np.array(list_of_data)[:,3:4].astype(np.float)))\n",
    "std=std/np.sqrt(num_bets_per_week)\n",
    "print 'mean: {}, std: {}'.format(mean,std)\n",
    "print 'doubling rate: {} weeks'.format(0.7/mean)\n",
    "x=np.linspace(mean-3*std,mean+3*std,100)\n",
    "plt.plot(x,mlab.normpdf(x,mean,std))\n",
    "plt.show()\n",
    "print 'probability of losing money: {}'.format(scipy.stats.norm(mean,std).cdf(0))\n",
    "print 'probability of losing half the money: {}'.format(scipy.stats.norm(mean,std).cdf(-0.5))\n",
    "print 'probability of losing all the money: {}'.format(scipy.stats.norm(mean,std).cdf(-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=0.087\n",
    "std=0.13\n",
    "prob_of_loss=0.01\n",
    "num_weeks=20\n",
    "percentage_of_money_bet=1\n",
    "money=1\n",
    "counter=0\n",
    "avg_ret=0\n",
    "testlength=1000\n",
    "for j in range(testlength):\n",
    "    money=1\n",
    "    i=0\n",
    "    while i <num_weeks:\n",
    "        i+=1\n",
    "        money+= percentage_of_money_bet*money*scipy.stats.norm.rvs(mean,std)\n",
    "        #print i,money\n",
    "    avg_ret+=money-1\n",
    "    if money<1:\n",
    "        counter+=1\n",
    "print avg_ret/float(testlength)\n",
    "print counter/float(testlength)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models=[]\n",
    "for file in glob.glob(\"/home/angloher/projects/Predicting_Winning_Teams/models/models*/NN_119934_*.pt\"):\n",
    "    best_models.append(torch.load(file))\n",
    "    \n",
    "test_set=torch.utils.data.TensorDataset(torch.from_numpy(X_test.as_matrix()).float(),torch.from_numpy(split_target(y_test['FTR'])).float())\n",
    "test_loader=torch.utils.data.DataLoader(test_set,batch_size=len(X_test),shuffle=False,num_workers=8)\n",
    "\n",
    "betting_data=y_test.drop(['FTR'],1).as_matrix()\n",
    "result_data=y_test['FTR'].as_matrix()\n",
    "\n",
    "threshs=[1]#np.linspace(0.5,2,10)\n",
    "alphas=[1]#np.linspace(-5,5,20)\n",
    "betting_tax=0\n",
    "verbose=False\n",
    "\n",
    "\n",
    "probas=np.zeros((len(betting_data),3),np.float)   \n",
    "\n",
    "for best_model in best_models:\n",
    "    probas=predict(test_loader,best_model)[0]\n",
    "    print best_model\n",
    "\n",
    "    for thresh in threshs:\n",
    "            #ret=[]\n",
    "        for alpha in alphas:\n",
    "            print thresh,alpha\n",
    "            data_count=0\n",
    "            won_overall=0\n",
    "            bets_overall=0\n",
    "            amount_overall=0\n",
    "            list_of_bets=[]\n",
    "            list_of_data=[]\n",
    "            for i in range(len(betting_data)):\n",
    "                won,num_bets,amount,list_of_bets,list_of_data=decide_on_bet(probas[i],betting_data[i],result_data[i],thresh,league,betting_tax,verbose,alpha,list_of_bets,list_of_data)\n",
    "                amount_overall+=amount\n",
    "                won_overall+=won\n",
    "                bets_overall+=num_bets\n",
    "\n",
    "                #ret.append(won_overall/float(amount_overall))\n",
    "            print 'average return on bets with overall is {}'.format(won_overall/float(amount_overall))\n",
    "            print 'won overall {} in {} bets'.format(won_overall,bets_overall)\n",
    "            print ''\n",
    "            print won_overall\n",
    "            print sum(list_of_bets)\n",
    "            print amount_overall\n",
    "            print sum(np.array(list_of_data)[:,3:4].astype(np.float))\n",
    "           # plt.plot(alphas,ret,label=str(thresh))\n",
    "        #plt.legend()\n",
    "        #plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
